---
title: "Comparison of Quantification methods for CD4 RNA-Seq Dataset"
author: "Ryan C. Thompson"
date: "`r gsub('\\s+', ' ', format(Sys.time(), '%B %e, %Y'))`"
output: html_notebook
params:
  basedir:
    value: "/home/ryan/Projects/CD4-csaw"
---

# Preliminary Setup

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(echo = TRUE, retina=2, cache=TRUE, autodep=TRUE,
                      cache.extra = list(params=params),
                      fig.height=8, fig.width=8,
                      cache.path = paste0(file.path(params$basedir, "cache", "rnaseq-compare"), .Platform$file.sep))
options(dplyr.show_progress=FALSE)
```

First we load the necessary libraries.

```{r load_packages, message=FALSE, cache=FALSE}
library(stringr)
library(magrittr)
library(openxlsx)
library(SummarizedExperiment)
library(dplyr)
library(edgeR)
library(limma)
library(sva)
library(ggplot2)
library(scales)
library(GGally)
library(ggalt)
library(reshape2)
library(assertthat)
library(ggfortify)
library(broom)

library(doParallel)
ncores <- getOption("mc.cores", default=parallel::detectCores(logical = FALSE))
registerDoParallel(cores=ncores)
library(BiocParallel)
register(DoparParam())
```

Next we define some utility functions.

```{r utility_functions, echo=FALSE}
# Useful to wrap functions that both produce a plot and return a useful value,
# when you only want the return value and not the plot.
suppressPlot <- function(arg) {
    png("/dev/null")
    result <- arg
    dev.off()
    result
}

ggprint <- function(plots, device=dev.cur(), closedev, printfun=print) {
  orig.device <- dev.cur()
  new.device <- device
  # Functions that create devices don't generally return them, they just set
  # them as the new current device, so get the actual device from dev.cur()
  # instead.
  if (is.null(new.device)) {
    new.device <- dev.cur()
  }
  if (missing(closedev)) {
    closedev = orig.device != new.device && new.device != 1
  }
  if (!is.null(device) && !is.na(device)) {
      dev.set(new.device)
      on.exit({
          if (closedev) {
              dev.off(new.device)
          }
          if (new.device != orig.device) {
              dev.set(orig.device)
          }
      })
  }
  assertthat::assert_that(is(plots, "gg") ||
                          is.list(plots))
  if (is(plots, "gg")) {
      printfun(plots)
  } else if (is.list(plots)) {
    lapply(plots, ggprint, device=NA, closedev=FALSE, printfun=printfun)
  } else {
    stop("Argument is not a ggplot or list of ggplots")
  }
  invisible(NULL)
}

# ggplotly.printer <- function(...) {
#     dots <- list(...)
#     function(p) {
#         args <- c(list(p=p), dots)
#         print(do.call(ggplotly, args))
#     }
# }

## Based on this: https://www.r-bloggers.com/shrinking-rs-pdf-output/
## Requires some command-line image and PDF manip tools.
rasterpdf <- function(pdffile, outfile=pdffile, resolution=600, verbose=FALSE) {
    vmessage <- function(...) {
        if (verbose) message(...)
    }
    require(parallel)
    wd=getwd()
    td=file.path(tempdir(),"pdf")
    if(!file.exists(td))
        dir.create(td)
    file.copy(pdffile, file.path(td,"test.pdf"))
    setwd(td)
    on.exit(setwd(wd))
    system2("pdftk", args=c("test.pdf", "burst"))
    files=list.files(pattern="pg_")

    vmessage(paste0("Rasterizing ",length(files)," pages:  (",paste(files,collapse=","),")"))
    bplapply(files,function(f){
        system2("gs", args=c("-dBATCH", "-dTextAlphaBits=4", "-dNOPAUSE",
                             paste0("-r", resolution), "-q", "-sDEVICE=png16m",
                             paste0("-sOutputFile=",f,".png"),f))
        system2("convert", args=c("-quality", "100", "-density", resolution,
                                  paste0(f,".png"),
                                  paste0(strsplit(f,".",fixed=T)[[1]][1],".pdf")))
        vmessage(paste0("Finished page ",f))
        return()
    })
    vmessage("Compiling the final pdf")
    file.remove("test.pdf")
    file.remove(list.files(pattern="png"))
    setwd(wd)
    system2("gs", args=c("-dBATCH", "-dNOPAUSE", "-q", "-sDEVICE=pdfwrite",
                         paste0("-sOutputFile=",outfile),
                         list.files(path=td, pattern=glob2rx("*.pdf"), full.names=TRUE)))
    file.remove(list.files(td,full=T))
    vmessage("Finished!!")
}

add.numbered.colnames <- function(x, prefix="C") {
    x %>% set_colnames(sprintf("%s%i", prefix, seq(from=1, length.out=ncol(x))))
}

autoFactorize <- function(df) {
    for (i in colnames(df)) {
        if (is.character(df[[i]]) && anyDuplicated(df[[i]])) {
            df[[i]] %<>% factor
        }
    }
    df
}

estimateDispByGroup <- function(dge, group=as.factor(dge$samples$group), batch, ...) {
    assert_that(nlevels(group) > 1)
    assert_that(length(group) == ncol(dge))
    if (!is.list(batch)) {
        batch <- list(batch=batch)
    }
    batch <- as.data.frame(batch)
    assert_that(nrow(batch) == ncol(dge))
    colnames(batch) %>% make.names(unique=TRUE)
    igroup <- seq_len(ncol(dge)) %>% split(group)
    bplapply(igroup, function(i) {
        group.dge <- dge[,i]
        group.batch <- droplevels(batch[i,, drop=FALSE])
        group.batch <- group.batch[sapply(group.batch, . %>% unique %>% length %>% is_greater_than(1))]
        group.vars <- names(group.batch)
        if (length(group.vars) == 0)
            group.vars <- "1"
        group.batch.formula <- as.formula(str_c("~", str_c(group.vars, collapse="+")))
        des <- model.matrix(group.batch.formula, group.batch)
        estimateDisp(group.dge, des, ...)
    })
}

ggduo.dataXY <- function(dataX, dataY, extraData=NULL, ...) {
    assert_that(ncol(dataX) > 0)
    assert_that(ncol(dataY) > 0)
    alldata <- cbind(dataX, dataY)
    if (!is.null(extraData)) {
        alldata <- cbind(alldata, extraData)
    }
    ggduo(alldata, columnsX=colnames(dataX), columnsY=colnames(dataY), ...)
}

# Make cairo_pdf use onefile=TRUE by default
cairo_pdf <- function(..., onefile=TRUE) {
    grDevices::cairo_pdf(..., onefile = onefile)
}

## Version of cpm that uses an offset matrix instead of lib sizes
cpmWithOffset <- function(dge, offset=expandAsMatrix(getOffset(dge), dim(dge)),
                          log = FALSE, prior.count = 0.25, ...) {
    x <- dge$counts
    effective.lib.size <- exp(offset)
    if (log) {
        prior.count.scaled <- effective.lib.size/mean(effective.lib.size) * prior.count
        effective.lib.size <- effective.lib.size + 2 * prior.count.scaled
    }
    effective.lib.size <- 1e-06 * effective.lib.size
    if (log)
        log2((x + prior.count.scaled) / effective.lib.size)
    else x / effective.lib.size
}

aveLogCPMWithOffset <- function(y, ...) {
    UseMethod("aveLogCPM")
}

aveLogCPMWithOffset.default <- function (y, offset = NULL, prior.count = 2,
                                         dispersion = NULL, weights = NULL, ...)
{
    aveLogCPM(y, lib.size = NULL, offset = offset, prior.count = prior.count, dispersion = dispersion, weights = weights, ...)
}

aveLogCPMWithOffset.DGEList <- function (
    y, offset = expandAsMatrix(getOffset(y), dim(y)),
    prior.count = 2, dispersion = NULL, ...) {
    if (is.null(dispersion)) {
        dispersion <- y$common.dispersion
    }
    aveLogCPMWithOffset(
        y$counts, offset = y$offset, prior.count = prior.count,
        dispersion = dispersion, weights = y$weights)
}


## Version of voom that uses an offset matrix instead of lib sizes
voomWithOffset <- function (
    dge, design = NULL, offset=expandAsMatrix(getOffset(dge), dim(dge)),
    normalize.method = "none", plot = FALSE, span = 0.5, ...)
{
    out <- list()
    out$genes <- dge$genes
    out$targets <- dge$samples
    if (is.null(design) && diff(range(as.numeric(counts$sample$group))) >
        0)
        design <- model.matrix(~group, data = counts$samples)
    counts <- dge$counts
    if (is.null(design)) {
        design <- matrix(1, ncol(counts), 1)
        rownames(design) <- colnames(counts)
        colnames(design) <- "GrandMean"
    }

    effective.lib.size <- exp(offset)

    y <- log2((counts + 0.5)/(effective.lib.size + 1) * 1e+06)
    y <- normalizeBetweenArrays(y, method = normalize.method)
    fit <- lmFit(y, design, ...)
    if (is.null(fit$Amean))
        fit$Amean <- rowMeans(y, na.rm = TRUE)
    sx <- fit$Amean + mean(log2(effective.lib.size + 1)) - log2(1e+06)
    sy <- sqrt(fit$sigma)
    allzero <- rowSums(counts) == 0
    if (any(allzero)) {
        sx <- sx[!allzero]
        sy <- sy[!allzero]
    }
    l <- lowess(sx, sy, f = span)
    if (plot) {
        plot(sx, sy, xlab = "log2( count size + 0.5 )", ylab = "Sqrt( standard deviation )",
            pch = 16, cex = 0.25)
        title("voom: Mean-variance trend")
        lines(l, col = "red")
    }
    f <- approxfun(l, rule = 2)
    if (fit$rank < ncol(design)) {
        j <- fit$pivot[1:fit$rank]
        fitted.values <- fit$coef[, j, drop = FALSE] %*% t(fit$design[,
            j, drop = FALSE])
    }
    else {
        fitted.values <- fit$coef %*% t(fit$design)
    }
    fitted.cpm <- 2^fitted.values
    ## fitted.count <- 1e-06 * t(t(fitted.cpm) * (lib.size + 1))
    fitted.count <- 1e-06 * fitted.cpm * (effective.lib.size + 1)
    fitted.logcount <- log2(fitted.count)
    w <- 1/f(fitted.logcount)^4
    dim(w) <- dim(fitted.logcount)
    out$E <- y
    out$weights <- w
    out$design <- design
    out$effective.lib.size <- effective.lib.size
    if (is.null(out$targets))
        out$targets <- data.frame(lib.size = exp(colMeans(offset)))
    else out$targets$lib.size <- exp(colMeans(offset))
    new("EList", out)
}

## Version of voom that uses an offset matrix instead of lib sizes
voomWithQualityWeightsAndOffset <-function (
    dge, design = NULL,
    offset=expandAsMatrix(getOffset(dge), dim(dge)),
    normalize.method = "none",
    plot = FALSE, span = 0.5, var.design = NULL, method = "genebygene",
    maxiter = 50, tol = 1e-10, trace = FALSE, replace.weights = TRUE,
    col = NULL, ...)
{
    counts <- dge$counts
    if (plot) {
        oldpar <- par(mfrow = c(1, 2))
        on.exit(par(oldpar))
    }
    v <- voomWithOffset(dge, design = design, offset = offset, normalize.method = normalize.method,
        plot = FALSE, span = span, ...)
    aw <- arrayWeights(v, design = design, method = method, maxiter = maxiter,
        tol = tol, var.design = var.design)
    v <- voomWithOffset(dge, design = design, weights = aw, offset = offset,
        normalize.method = normalize.method, plot = plot, span = span,
        ...)
    aw <- arrayWeights(v, design = design, method = method, maxiter = maxiter,
        tol = tol, trace = trace, var.design = var.design)
    wts <- asMatrixWeights(aw, dim(v)) * v$weights
    attr(wts, "arrayweights") <- NULL
    if (plot) {
        barplot(aw, names = 1:length(aw), main = "Sample-specific weights",
            ylab = "Weight", xlab = "Sample", col = col)
        abline(h = 1, col = 2, lty = 2)
    }
    if (replace.weights) {
        v$weights <- wts
        v$sample.weights <- aw
        return(v)
    }
    else {
        return(wts)
    }
}

# plotdir <- file.path(params$basedir, "plots/RNA-seq", params$dataset)
# dir.create(plotdir, recursive = TRUE, showWarnings = FALSE)
```

# Data Loading and Preprocessing

Now we'll load the RNA-seq data set from an RDS file containing a SummarizedExperiment object, and modify it to use the sample names as column names.

```{r load_data}
datasets <- c("hisat2_grch38_snp_tran_ensembl.85",
              "hisat2_grch38_snp_tran_knownGene",
              "kallisto_hg38.analysisSet_ensembl.85",
              "kallisto_hg38.analysisSet_knownGene",
              "salmon_hg38.analysisSet_ensembl.85",
              "salmon_hg38.analysisSet_knownGene",
              "shoal_hg38.analysisSet_ensembl.85",
              "shoal_hg38.analysisSet_knownGene",
              "star_hg38.analysisSet_ensembl.85",
              "star_hg38.analysisSet_knownGene")
datasetmeta <- datasets %>% str_match("^([^_]+)_(.+)_([^_]+)$") %>%
    as_data_frame %>% setNames(c("dataset", "program", "genome", "transcriptome"))
assert_that(all(table(datasetmeta$program) == 2))
assert_that(all(table(datasetmeta$transcriptome) == 5))
sexp.files <- setNames(file.path(params$basedir, "saved_data",
                                sprintf("SummarizedExperiment_rnaseq_%s.RDS", datasets)),
                      datasets)
sexps <- bplapply(sexp.files, readRDS)
for (dset in names(sexps)) {
    sexps[[dset]] %<>% set_colnames(.$SampleName)
}
```

We extract the sample metadata from the SummarizedExperiment (it's the same for all datasets). Since donor_id is a confounding factor, we tell R to use sum-to-zero contrasts when incorporating it into a design matrix.

```{r extract_samplemeta}
sample.table <- colData(sexps[[1]]) %>%
    as.data.frame %>% autoFactorize %>%
    rename(batch=technical_batch) %>%
    mutate(time_point=factor(days_after_activation) %>% `levels<-`(sprintf("D%s", levels(.))),
           group=interaction(cell_type, time_point, sep=""))
contrasts(sample.table$donor_id) <- contr.sum(nlevels(sample.table$donor_id))
```

Next we extract the count matrix from each SummarizedExperiment. This is made more complicated than usual by the fact that half of the samples were sequenced with a different protocol than the other half, and the two protocols produce reads with opposite strand orientations. Hence, we need the sense counts for half of the samples and the antisense counts for the other half. The appropriate strand for each sample is documented in the `libType` column of the sample metadata, using the library type abbreviations [established by Salmon](http://salmon.readthedocs.io/en/latest/salmon.html#what-s-this-libtype).

For SummarizedExperiments generated using tximport, this step is skipped, since the quantification tool has already been told which strand to use and only provides counts for that strand.

```{r extract_counts}
libtype.assayNames <- c(SF="sense.counts", SR="antisense.counts")
for (dset in seq_along(sexps)) {
    if (all(libtype.assayNames %in% assayNames(sexps[[dset]]))) {
        sample.table %<>% mutate(count_type=libtype.assayNames[libType])
        assay(sexps[[dset]], "unstranded.counts") <- assay(sexps[[dset]], "counts")
        assay(sexps[[dset]], "counts") <- lapply(seq_len(nrow(sample.table)), function(i) {
            assay(sexps[[dset]], sample.table[i,]$count_type %>% as.character)[,i]
        }) %>% do.call(what=cbind)
    }
}
```

As a sanity check, we make sure that we selected the strand sense with the higher total count for each sample.

```{r strand_sanity_check, message=FALSE}
for (dset in seq_along(sexps)) {
    if (all(libtype.assayNames %in% assayNames(sexps[[dset]]))) {
        total.counts <- sexps[[dset]] %>% assays %>% sapply(colSums) %>% data.frame %>%
            mutate(SampleName=row.names(.)) %>%
            inner_join(sample.table, by="SampleName")
        total.counts %$% invisible(assert_that(all(counts == pmax(sense.counts, antisense.counts))))
    }
}
```

Now we create a DGEList from each count matrix, and perform the initial scaling normalization. In addition, if there is a length assay, we also use that to derive an offset matrix that corrects for sample-specific biases detected by Salmon or shoal.

```{r prepare_dgelist}
dges <- bplapply(sexps, function(sexp) {
    ## Extract gene metadata and colapse lists
    all.gene.meta <- mcols(sexp) %>% as.data.frame
    # Convert list columns to character vectors
    all.gene.meta[] %<>% lapply(function(x) if (is.list(x)) sapply(x, str_c, collapse=",") else x)
    dge <- DGEList(counts=assay(sexp, "counts"))
    dge$genes <- all.gene.meta
    rownames(dge$genes) <- rownames(dge)
    dge %<>% calcNormFactors
    if ("length" %in% assayNames(sexp)) {
        normMat <- assay(sexp, "length") %>% divide_by(exp(rowMeans(log(.))))
        normCounts <- dge$counts/normMat
        lib.offsets <- log(calcNormFactors(normCounts)) + log(colSums(normCounts))
        dge$offset <- t(t(log(normMat)) + lib.offsets)
    }
    dge
})
```

Finally, we compute the normalized logCPM matrix for each dataset, and convert it into a tidy format. We also compute the normalized logCPM without taking into account the offsets derived from gene length normalization. 

```{r prepare_logCPM, message=FALSE}
logcpm.matrices <- bplapply(dges, cpmWithOffset, log=TRUE)
logcpm.matrices.nooffset <- bplapply(dges, cpm, log=TRUE)
logcpm <- bplapply(dges, function(dge) {
    cpm.mat <- cpmWithOffset(dge, log=TRUE)
    cpm.df <- melt(cpm.mat, varnames=c("GeneID", "Sample"), value.name = "logCPM") %>%
        mutate(GeneID=as.character(GeneID), Sample=as.character(Sample))
    genes <- dge$genes %>% mutate(GeneID=rownames(.))
    inner_join(genes[c("GeneID", "ENTREZID", "ENSEMBL")], cpm.df, by="GeneID") %>%
        select(-GeneID)
})
logcpm.nooffset <- bplapply(dges, function(dge) {
    cpm.mat <- cpm(dge, log=TRUE)
    cpm.df <- melt(cpm.mat, varnames=c("GeneID", "Sample"), value.name = "logCPM") %>%
        mutate(GeneID=as.character(GeneID), Sample=as.character(Sample))
    genes <- dge$genes %>% mutate(GeneID=rownames(.))
    inner_join(genes[c("GeneID", "ENTREZID", "ENSEMBL")], cpm.df, by="GeneID") %>%
        select(-GeneID)
})
```
    
# Comparative Analysis
    
Now we can begin comparing the different methods of quantification. First, we will compare Ensembl vs UCSC annotations for each quantification method. 
    
## Ensembl vs UCSC
    
First, we format the logCPM tables into tidy format, and merge the tables of both datasets (Ensembl and UCSC knownGene) for each alignment/quantification program. The merging is performed only for genes with a one-to-one relationship between Ensembl and Entrez gene IDs.
    
```{r compute_ensembl_vs_ucsc, message=FALSE}
logcpm.tables <- datasetmeta %>%
    arrange(transcriptome) %>%
    group_by(program) %>% do({
        df <- .
        assert_that(nrow(df) == 2)
        dsetA <- df$dataset[1]
        dsetB <- df$dataset[2]
        txA <- df$transcriptome[1]
        txB <- df$transcriptome[2]
        logcpm_A <- logcpm[[dsetA]] %>% 
            rename(logCPM_A=logCPM) %>%
            filter(lengths(ENTREZID) == 1, lengths(ENSEMBL) == 1) %>%
            mutate(ENTREZID=unlist(ENTREZID),
                   ENSEMBL=unlist(ENSEMBL))
        logcpm_B <- logcpm[[dsetB]] %>%
            rename(logCPM_B=logCPM) %>%
            filter(lengths(ENTREZID) == 1, lengths(ENSEMBL) == 1) %>%
            mutate(ENTREZID=unlist(ENTREZID),
                   ENSEMBL=unlist(ENSEMBL))
        logcpm_merged <- inner_join(logcpm_A, logcpm_B)
        data_frame(program=df$program[1], 
                   genome=df$genome[1],
                   datasetA=dsetA, datasetB=dsetB,
                   transcriptomeA=txA, transcriptomeB=txB,
                   logcpm=list(logcpm_merged))
    })
# Print the correlations
logcpm.tables %>% group_by_(names(.)[1:5]) %>% do({
    data.frame(Correlation=.$logcpm[[1]] %$% cor(logCPM_A, logCPM_B))
})
```

Now we plot the logCPM for Ensembl vs UCSC, for each quantification method (program + transcriptome).

```{r plot_ensembl_vs_ucsc}
# Use the same x and y limits for all plots
limits <- logcpm.tables$logcpm %>% lapply(. %$% c(range(logCPM_A), range(logCPM_B))) %>% unlist %>% range
logcpm.plots <- logcpm.tables %>% group_by(program) %>% do({
    df <- .
    assert_that(nrow(df) == 1)
    xlabel <- str_c("Gene logCPM in ", df$datasetA)
    ylabel <- str_c("Gene logCPM in ", df$datasetB)
    ptitle <- sprintf("logCPM, %s vs %s for %s on %s genome", df$transcriptomeA, df$transcriptomeB, df$program, df$genome)
    tab <- df$logcpm[[1]] %>% filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
    p <- ggplot(tab) + 
        aes(x=logCPM_A, y=logCPM_B) +
        geom_point(size=0.2, alpha=0.2) +
        geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
        coord_fixed(xlim=limits, ylim=limits) +
        labs(title=ptitle, 
             subtitle="Genes matched by unique Entrez & Ensembl IDs; identity line (red) plotted red for reference",
             x=xlabel, y=ylabel)
    cbind(.[c("program", "genome", "datasetA", "datasetB", "transcriptomeA", "transcriptomeB")],
          data_frame(plot=list(p)))
})
ggprint(logcpm.plots$plot)
```

Overall concordance is very high for all methods. The alignment-based methods (STAR and HISAT2) have slightly higher correlation, which is matched by the apparently tighter spread of points about the identity line in the plots. This is expected since these methods discard ambiguous reads, and are therefore biased toward unambiguously-defined genes, which are the same genes more likely to be annotated consistently between Entrez and Ensembl.

Interestingly, the correlation for Shoal is lower than for Salmon, despite the fact that the Shoal plot looks like it has a tighter spread and the fact that Shoal is meant to regularize Salmon's quantification results, which would be expected to increase the correlation. Perhaps Shoal decreases the overall correlation by spreading many points away from the bottom left corner, where previously all the genes with zero counts were exactly overlaid on a single point. (This kind of effect is why pearson correlations on gene abundances is rarely informative.) 

## STAR vs HISAT2

The two count-based methods differ only in the aligner that was used, so any discrepancy between them represents a difference in which reads they could align unambiguously.

```{r plot_star_vs_hisat2, message=FALSE}
logcpm_A <- logcpm$hisat2_grch38_snp_tran_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_A=logCPM)
logcpm_B <- logcpm$star_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_B=logCPM)
logcpm.merged <- inner_join(logcpm_A, logcpm_B) %>%
    filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
limits <- logcpm.merged %>% select(logCPM_A, logCPM_B) %>% lapply(range) %>% unlist %>% range
p <- ggplot(logcpm.merged) +
    aes(x=logCPM_A, y=logCPM_B) +
    geom_point(size=0.2, alpha=0.2) +
    geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
    coord_fixed(xlim=limits, ylim=limits) +
    labs(title="logCPM, STAR vs HISAT2", 
         subtitle="Identity line (red) plotted red for reference",
         x="logCPM by HISAT2", y="logCPM by STAR")
ggprint(p)
```
## Effect of gene length normalization

We now compare Salmon, Shoal, and Kallisto with and without normalizing for gene length. This will show how much or how little the gene length normalization affects the abundance estimation.

```{r plot_length_norm_effect, message=FALSE}
logcpm.plots <- datasetmeta %>%
    filter(transcriptome == "ensembl.85", program %in% c("salmon", "shoal", "kallisto")) %>% 
    group_by(dataset, program) %>% do({
        assert_that(nrow(.) == 1)
        dset <- .$dataset
        program <- .$program
        logcpm_A <- logcpm.nooffset[[dset]] %>% 
            select(ENSEMBL, Sample, logCPM) %>%
            rename(logCPM_A=logCPM)
        logcpm_B <- logcpm[[dset]] %>% 
            select(ENSEMBL, Sample, logCPM) %>%
            rename(logCPM_B=logCPM)
        logcpm.merged <- inner_join(logcpm_A, logcpm_B) %>%
            filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
        limits <- logcpm.merged %>% select(logCPM_A, logCPM_B) %>% lapply(range) %>% unlist %>% range
        p <- ggplot(logcpm.merged) +
            aes(x=logCPM_A, y=logCPM_B) +
            geom_point(size=0.2, alpha=0.2) +
            geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
            coord_fixed(xlim=limits, ylim=limits) +
            labs(title=sprintf("%s logCPM, length-normalized vs non-length-normalized", program), 
                 subtitle="Identity line (red) plotted red for reference",
                 x="logCPM without gene length normalization", y="logCPM with gene length normalization")
        data_frame(plot=list(p))
    })
ggprint(logcpm.plots$plot)
```

## STAR vs Salmon

We now compare STAR, a count-based method, to Salmon, an alignment-free method. We use the Salmon results with no gene length normalization for an apples-to-apples comparison.

```{r plot_star_vs_salmon, message=FALSE}
logcpm_A <- logcpm$star_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_A=logCPM)
logcpm_B <- logcpm.nooffset$salmon_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_B=logCPM)
logcpm.merged <- inner_join(logcpm_A, logcpm_B) %>%
    filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
limits <- logcpm.merged %>% select(logCPM_A, logCPM_B) %>% lapply(range) %>% unlist %>% range
p <- ggplot(logcpm.merged) +
        aes(x=logCPM_A, y=logCPM_B) +
        geom_point(size=0.2, alpha=0.2) +
        geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
        coord_fixed(xlim=limits, ylim=limits) +
        labs(title="logCPM, Salmon vs STAR", 
             subtitle="Identity line (red) plotted red for reference",
             x="logCPM by STAR", y="logCPM by Salmon")
ggprint(p)
```

## Salmon vs Kallisto

```{r plot_salmon_vs_kallisto, message=FALSE}
logcpm_A <- logcpm$kallisto_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_A=logCPM)
logcpm_B <- logcpm$salmon_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_B=logCPM)
logcpm.merged <- inner_join(logcpm_A, logcpm_B) %>%
    filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
limits <- logcpm.merged %>% select(logCPM_A, logCPM_B) %>% lapply(range) %>% unlist %>% range
p <- ggplot(logcpm.merged) +
        aes(x=logCPM_A, y=logCPM_B) +
        geom_point(size=0.2, alpha=0.2) +
        geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
        coord_fixed(xlim=limits, ylim=limits) +
        labs(title="logCPM, Salmon vs Kallisto", 
             subtitle="Identity line (red) plotted red for reference",
             x="logCPM by Kallisto", y="logCPM by Salmon")
ggprint(p)
```

## Salmon vs Shoal

```{r plot_shoal_vs_salmon, message=FALSE}
logcpm_A <- logcpm$salmon_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_A=logCPM)
logcpm_B <- logcpm$shoal_hg38.analysisSet_ensembl.85 %>% 
    select(ENSEMBL, Sample, logCPM) %>%
    rename(logCPM_B=logCPM)
logcpm.merged <- inner_join(logcpm_A, logcpm_B) %>%
    filter(logCPM_A != min(logCPM_A) | logCPM_B != min(logCPM_B))
limits <- logcpm.merged %>% select(logCPM_A, logCPM_B) %>% lapply(range) %>% unlist %>% range
p <- ggplot(logcpm.merged) +
        aes(x=logCPM_A, y=logCPM_B) +
        geom_point(size=0.2, alpha=0.2) +
        geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
        coord_fixed(xlim=limits, ylim=limits) +
        labs(title="logCPM, Shoal vs Salmon", 
             subtitle="Identity line (red) plotted red for reference",
             x="logCPM by Salmon", y="logCPM by Shoal")
ggprint(p)
```
